# International quotes

This is a project to Internet applications (WAP) course at FIT BUT.

- Author: Michal Šmahel (xsmahe01)
- Date: April 2025

All the code in this repository (or submitted archive) is my own work unless otherwise stated. Documentation comments
were generated by a language model from Jetbrains AI Assistant, checked and edited to the final form by me. Some parts
of used core skeleton code come from sources listed in the following section (mainly from chats with language models).
All of these were checked and edited to the final form by me.

Generated documentation is available under the `doc` directory. It is created by `typedoc`.


## Used and studied sources (relevant only)

This section contains links to the sources that were used in the project. For limiting the list size, only relevant
(actually used or planned to be used) sources are listed.

### General

This subsection lists links to related sources to the project, which are used by both parts of the project.

- [README of `nvm-sh`](https://github.com/nvm-sh/nvm)
- [Documentation of `tsconfig.js`](https://www.typescriptlang.org/tsconfig/)
- [README of `docker-node`](https://github.com/nodejs/docker-node/blob/main/README.md)
- Source code of the 1st project to WAP
- [Article about setting up `ts-node` with `dotenv`](https://medium.com/@drewdrewthis/running-node-typescript-scripts-typedi-decorator-metadata-with-dotenv-env-vars-in-your-nextjs-413374426882)
- [Documentation of `typedi`](https://docs.typestack.community/typedi)
- [Documentation of `typeorm`](https://typeorm.io/)
- [Article about exceptions (errors, resp.) in TypeScript](https://www.dhiwise.com/post/typescript-error-handling-pitfalls-and-how-to-avoid-them)
- [Discussion about uniqueness in TypeORM](https://stackoverflow.com/a/66375522)

### Dataset building

This subsection lists links to sources related to the dataset building from Wiki quote pages.

- [Documentation of Wikimedia dumps](https://dumps.wikimedia.org/)
- [README of `fast-xml-parser`](https://github.com/NaturalIntelligence/fast-xml-parser)
- [README of `wtf_wikipedia`](https://github.com/spencermountain/wtf_wikipedia)
- [Pricing of Google language model API](https://ai.google.dev/gemini-api/docs/pricing)
- [Documentation of Google AI API](https://googleapis.github.io/js-genai/main/index.html)
- [Article about type for associative array in TypeScript](https://www.geeksforgeeks.org/how-to-use-associative-array-in-typescript/)
- [Article about reading files via node.js](https://nodejs.org/en/learn/manipulating-files/reading-files-with-nodejs)

### API

This subsection lists links to sources related to the API part of the project.

- [Documentation of `express`](https://expressjs.com/)
- [Lecture materials for `express`](https://www.fit.vut.cz/study/course/WAP/private/lectures/2025.php?p=backend#/6)
- [README of `routing-controllers`](https://github.com/typestack/routing-controllers)
- [README of `class-validator`](https://github.com/typestack/class-validator)

### Documentation

This subsection lists links to sources related to the documentation of the project.

- [Documentation of `typedoc`](https://typedoc.org/modules.html)
- [README of `typedoc-theme-hierarchy`](https://github.com/DiFuks/typedoc-theme-hierarchy)

# Static index page

The subsection lists links to sources related to the static index page of the project:

- [StackOverflow discussion about CORS with `helmet`](https://stackoverflow.com/a/69902361)
- [SRI Hash Generator](https://www.srihash.org/)
- [MDN web docs about resource integrity](https://developer.mozilla.org/en-US/docs/Web/Security/Subresource_Integrity)
- [Anthropic's documentation about language models](https://docs.anthropic.com/en/docs/about-claude/models/all-models#model-comparison-table)

### Used conversations with language models

These conversations with a language model were used for observing the project domain and solving some problems:

- https://chat.ceskydj.cz/share/hc1grxuvQIa-putJH131L
- https://chat.ceskydj.cz/share/9sV1mc8SU8KaT4rY19LpT
- https://chat.ceskydj.cz/share/taPSfJqQrMEcn2r4onAvN
- https://chat.ceskydj.cz/share/RLQkR21M4er5gs7QJsX0R
- https://chat.ceskydj.cz/share/WsLY4uBDTC6zyWXlLdPlz
- https://chat.ceskydj.cz/share/4b9TfGb9uIwHm6yEk_wFd
- https://chat.ceskydj.cz/share/EeDqzSiG1QssANaBCRoJb
- https://chat.ceskydj.cz/share/XVe_JMTs0A661zKMPRG9J
- https://chat.ceskydj.cz/share/yeLL6npy1cvR7aLKB76jH


## Prepared prompts for processing unstructured data from Wiki quote pages

There are several prepared prompts for processing unstructured data from Wiki quote pages:

- [Human name checker and normalizer](https://aistudio.google.com/app/prompts?state=%7B%22ids%22:%5B%2210oDw_hBQn2rnOb-LvQqvCDYYPwgZZ0wD%22%5D,%22action%22:%22open%22,%22userId%22:%22105912846041291166242%22,%22resourceKeys%22:%7B%7D%7D&usp=sharing)
- [Quote scoring](https://aistudio.google.com/app/prompts?state=%7B%22ids%22:%5B%221-FZltVKD-qjx0JkhogYeXCLOBaop9mzH%22%5D,%22action%22:%22open%22,%22userId%22:%22105912846041291166242%22,%22resourceKeys%22:%7B%7D%7D&usp=sharing)


## Idea behind the project

As there are only a few international quote APIs available on the Internet, and as far as I know, none of them are free
(price and freedom) to use, I decided to create my own API.

The data comes from the [Wiki quote](https://www.wikiquote.org/), which is another project from Wikimedia Foundation
(as Wikipedia, Wikidata and other free sources of knowledge). The decision to use this source was made in
https://github.com/XengShi/materialYouNewTab/issues/457 as Wiki quote has free license and contains quotes from many
languages. On top of that, Wiki quote offers dump files, which could be processed without scraping the website.

There is a challenge to process the data from Wiki quote, as they aren't very structured (which isn't too obvious at
first sight). Every language has its own way of how to structure pages, and none of them uses very strict structure
(there are lots of subcategories, which are defined by natural language, so unsourced quotes are combined with
motivational, etc.).

I figured out that the best way to process the data is to use a language model, which can handle unstructured data
pretty well. I decided to use the Google Gemini API, which is a paid service, but it has a free budget for start,
which should be enough for the project.

Language models are used for:
- **detecting pages about people**—There are lots of aggregation pages that combine quotes from several people and group
them to some categories. This is done by checking if the page title consists of a human name.
- **normalizing author names**—Each language names the same person differently, so if I want to group quotes by author
no matter the language, I need to normalize the names to English form.
- **scoring quotes**—As pages are unstructured, I need to consider what is a quote and what is some descriptive text
and finally recognize good-quality quotes people care about the most. This is done by assigning an integer score to each
extracted potential quote and comparing the score against a threshold.

After processing, the data is stored in a relational database (PostgreSQL) and served by a simple REST API.

Updating process is planned to be done as a cron job, which will check if there are new dump files and process them. As
it's hard to predict what quotes were changed, this process just cleans the database and processes all the dump files
again. This is not the most efficient way, but it shouldn't be a problem, as the dump files are updated pretty rarely
(it should be about once a month).
